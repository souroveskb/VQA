{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from normalizer import normalize # pip install git+https://github.com/csebuetnlp/normalizer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"আমি কৃতজ্ঞ কারণ আপনি আমার জন্য অনেক কিছু\"\n",
    "fake_sentence = \"আপনি হতাশ কারণ  আমার জন্য অনেক কিছু করেছেন।\"\n",
    "fake_sentence = normalize(fake_sentence) # this normalization step is required before tokenizing the text\n",
    "original_sentence = normalize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tokens = tokenizer.tokenize(fake_sentence)\n",
    "fake_inputs = tokenizer.encode_plus(fake_sentence, return_tensors=\"pt\")\n",
    "# fake_inputs\n",
    "\n",
    "original_inputs = tokenizer.encode_plus(original_sentence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_outputs = model(fake_inputs['input_ids'],attention_mask= fake_inputs['attention_mask'])#.logits\n",
    "discriminator_outputs2 = model(original_inputs['input_ids'],attention_mask= original_inputs['attention_mask'])#.logits\n",
    "# predictions = torch.round((torch.sign(discriminator_outputs) + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = discriminator_outputs['last_hidden_state'][:,0,:]\n",
    "input2 = discriminator_outputs2['last_hidden_state'][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8526], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cos(input1, input2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31490/2331687903.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_train = pd.read_csv(\"dataset/bornon_train_token.txt\", delimiter=\"#0\\s+\", names=[\"image_name\", \"caption\"], header=None)\n",
      "/tmp/ipykernel_31490/2331687903.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_test = pd.read_csv(\"dataset/bornon_test_token.txt\", delimiter=\"#0\\s+\", names=[\"image_name\", \"caption\"], header=None)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"dataset/bornon_train_token.txt\", delimiter=\"#0\\s+\", names=[\"image_name\", \"caption\"], header=None)\n",
    "df_test = pd.read_csv(\"dataset/bornon_test_token.txt\", delimiter=\"#0\\s+\", names=[\"image_name\", \"caption\"], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(gdf):\n",
    "    captions = gdf['caption'].to_list()\n",
    "    return captions\n",
    "\n",
    "df1 = df_train.groupby(\"image_name\").apply(consolidate, include_groups=False).reset_index().rename(columns={0: 'captions'})\n",
    "df2 = df_test.groupby(\"image_name\").apply(consolidate, include_groups=False).reset_index().rename(columns={0: 'captions'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by='image_name', key=lambda x: pd.to_numeric(x.str.rstrip('.jpg'), errors='coerce'), inplace=True)\n",
    "df2.sort_values(by='image_name', key=lambda x: pd.to_numeric(x.str.rstrip('.jpg'), errors='coerce'), inplace=True)\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_captions(captions: list):\n",
    "    \"\"\"\n",
    "    top 3 being the max length string, minimum length string and the string which has maximum cosine similarity index with \n",
    "    both of the max and min length string\n",
    "\n",
    "    Args:\n",
    "        captions (list): list of 5 captions from the bornon dataset for each image\n",
    "\n",
    "    Returns:\n",
    "        _type_: list of top 3 captions\n",
    "    \"\"\"\n",
    "    cap_discriminator_outputs = []\n",
    "    cap_inputs = []\n",
    "    cap_final = []\n",
    "    \n",
    "    # cap_final.append(tokenizer.encode_plus(normalize(max(captions, key=len)), return_tensors=\"pt\"))\n",
    "    # cap_final.append(tokenizer.encode_plus(normalize(min(captions, key=len)), return_tensors=\"pt\"))\n",
    "    cap_final.append(normalize(max(captions, key=len)))  \n",
    "    cap_final.append(normalize(min(captions, key=len)))  \n",
    "    \n",
    "    for i in range(len(captions)):\n",
    "        normalised = normalize(captions[i])\n",
    "        cap_inputs.append(tokenizer.encode_plus(normalised, return_tensors=\"pt\"))\n",
    "    \n",
    "    for i in range(len(cap_inputs)):\n",
    "        cap_discriminator_outputs.append(model(cap_inputs[i]['input_ids'],attention_mask= cap_inputs[i]['attention_mask'])['last_hidden_state'][:,0,:])\n",
    "    \n",
    "    max_len_str = tokenizer.encode_plus(cap_final[0], return_tensors=\"pt\")\n",
    "    min_len_str = tokenizer.encode_plus(cap_final[1], return_tensors=\"pt\")\n",
    "    \n",
    "    max_len_str = model(max_len_str['input_ids'],attention_mask= max_len_str['attention_mask'])['last_hidden_state'][:,0,:]\n",
    "    min_len_str = model(min_len_str['input_ids'],attention_mask= min_len_str['attention_mask'])['last_hidden_state'][:,0,:]\n",
    "    \n",
    "    \n",
    "    # print(max_len_str.shape)\n",
    "    \n",
    "    max_val = -1\n",
    "    idx = 0\n",
    "    for i in range(len(cap_discriminator_outputs)):\n",
    "        # print(i)\n",
    "        if cos(max_len_str, cap_discriminator_outputs[i]) > 0.95 or cos(min_len_str, cap_discriminator_outputs[i]) > 0.95:\n",
    "            # print(captions[i], cap_final[0], cap_final[1])\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            val1 = cos(max_len_str, cap_discriminator_outputs[i])\n",
    "            val2 = cos(min_len_str, cap_discriminator_outputs[i])\n",
    "            \n",
    "            # print(captions[i])\n",
    "            # print(captions[i], cap_final[0], cap_final[1])\n",
    "            \n",
    "            if val1+val2 > max_val:\n",
    "                max_val = val1+val2\n",
    "                idx = i\n",
    "                \n",
    "    cap_final.append(normalize(captions[idx]))\n",
    "    \n",
    "    return (cap_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top_captions'] = None\n",
    "from tqdm import notebook as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a561e32f0c2a48b1a4a0ed9da7fafb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, row in nb.tqdm(df.iterrows()):\n",
    "    \n",
    "    outs = top_3_captions(captions=row['captions'])\n",
    "    # df.loc[idx, 'top_captions'] = ',\\n '.join(map(str, outs))\n",
    "    \n",
    "    df.loc[df['image_name'] == row['image_name'], 'top_captions'] = ',\\n '.join(map(str, outs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_name</th>\n",
       "      <th>captions</th>\n",
       "      <th>top_captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>[একটি পার্কে একটি লেক আছে ।, লেকের উপর একটি ব্...</td>\n",
       "      <td>গাছপালায় ঘেরা এই পার্কটি খুবই মনমুগ্ধকর ।,\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>934</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>[একজন মানুষ মাথায় খড় নিয়ে হেটে যাচ্ছে ।, এক...</td>\n",
       "      <td>লুঙ্গি পড়া একজন মানুষ খড়ের উপর হেটে যাচ্ছে ।...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1885</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>[টেবিলের উপর একটি চশমা আছে ।, টেবিলের উপর একটি...</td>\n",
       "      <td>একটি রুমে একটি খাট এবং একটি টেবিল আছে,টেবিলের ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2829</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>[একটি শিশু দেখা যাচ্ছে ।, একটি শিশু গ্রামের ছো...</td>\n",
       "      <td>একটি শিশু হাটার সময় তার কিছুটা ছায়া দেখা যাচ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3013</td>\n",
       "      <td>5.jpg</td>\n",
       "      <td>[একটি চায়ের কাপ দেখা যাচ্ছে ।, চায়ের কাপে উপ...</td>\n",
       "      <td>আলোর কারনে চায়ের কাপের ছায়া দেখা যাচ্ছে ।,\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index image_name                                           captions  \\\n",
       "0      0      1.jpg  [একটি পার্কে একটি লেক আছে ।, লেকের উপর একটি ব্...   \n",
       "1    934      2.jpg  [একজন মানুষ মাথায় খড় নিয়ে হেটে যাচ্ছে ।, এক...   \n",
       "2   1885      3.jpg  [টেবিলের উপর একটি চশমা আছে ।, টেবিলের উপর একটি...   \n",
       "3   2829      4.jpg  [একটি শিশু দেখা যাচ্ছে ।, একটি শিশু গ্রামের ছো...   \n",
       "4   3013      5.jpg  [একটি চায়ের কাপ দেখা যাচ্ছে ।, চায়ের কাপে উপ...   \n",
       "\n",
       "                                        top_captions  \n",
       "0  গাছপালায় ঘেরা এই পার্কটি খুবই মনমুগ্ধকর ।,\\n ...  \n",
       "1  লুঙ্গি পড়া একজন মানুষ খড়ের উপর হেটে যাচ্ছে ।...  \n",
       "2  একটি রুমে একটি খাট এবং একটি টেবিল আছে,টেবিলের ...  \n",
       "3  একটি শিশু হাটার সময় তার কিছুটা ছায়া দেখা যাচ...  \n",
       "4  আলোর কারনে চায়ের কাপের ছায়া দেখা যাচ্ছে ।,\\n...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"top_captioned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
